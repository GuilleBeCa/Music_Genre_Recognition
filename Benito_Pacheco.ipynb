{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5c09ff-9a1a-484e-b17a-40a4a2c78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam, Adadelta\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils  ## .py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f22add-bef1-484a-9315-1575b56ce703",
   "metadata": {},
   "source": [
    "### Load meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fd8af6-fd88-4a09-8f1c-fb591baabff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106574, 52), (163, 4), (106574, 518), (13129, 249))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory where mp3 are stored.  ### in terminal write: export AUDIO_DIR=./'directory where the mp3 are stored'\n",
    "AUDIO_DIR=os.environ.get('AUDIO_DIR')\n",
    "\n",
    "# Load metadata and features.\n",
    "tracks_ = utils.load('fma/data/fma_metadata/tracks.csv')\n",
    "genres = utils.load('fma/data/fma_metadata/genres.csv')\n",
    "features = utils.load('fma/data/fma_metadata/features.csv')\n",
    "echonest = utils.load('fma/data/fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks_.index)\n",
    "assert echonest.index.isin(tracks_.index).all()\n",
    "\n",
    "tracks_.shape, genres.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ee858-041c-4bc9-8e27-e46e5c397cdc",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa75045-fbea-4ca3-b1c0-b97a29d9ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Possible trainings\n",
    "model_dim = 'both' ## '1d' or '2d' 'both'\n",
    "Division = True ## if data gets diveded in small pieces\n",
    "Drop_out = False\n",
    "\n",
    "N = 7994 ## number of data samples we will work with (max = 7994)\n",
    "batch_size = 50\n",
    "epochs = 15\n",
    "lr = 5e-4\n",
    "\n",
    "model_saving_name = 'model.pt'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1330bd-6278-492b-80f0-e8c63dc13049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data 1D: (1,142857)\n",
      "Sape of data 2D: (1025, 139)\n"
     ]
    }
   ],
   "source": [
    "max_len = 1_000_000  ## max = 1_321_438\n",
    "n_fft = 2048  ## for the STFT fourier transform\n",
    "n_rows = int(np.round((n_fft+1.1)/2))\n",
    "hop_length = n_fft//2  ## n_fft/a  1/a = overlap\n",
    "n_columns = int(np.ceil(max_len/hop_length))\n",
    "if Division == True:\n",
    "    num_divisions = 7\n",
    "    stride_calc = 919 \n",
    " #int(max_len/(N+19))-200\n",
    "\n",
    "elif Division == False:\n",
    "    num_divisions = 1\n",
    "    stride_calc = 1007\n",
    "\n",
    "if Drop_out == True:\n",
    "    drop_out = 0.2\n",
    "elif Drop_out == False:\n",
    "    drop_out = 0\n",
    "\n",
    "\n",
    "n_columns_division = int(n_columns/num_divisions)\n",
    "n_data_division = int(max_len/num_divisions)\n",
    "\n",
    "print(f'Shape of data 1D: (1,{n_data_division})')\n",
    "print(f'Sape of data 2D: {n_rows,n_columns_division}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0186bc-87aa-49de-ad60-886d137568f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data \"preprocessing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655a23d6-03cf-4ee2-89e7-5cf194bc1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUBSETING , we are only going to work with the small data_set and oly with N samples.\n",
    "\n",
    "subset = tracks_.index[tracks_['set', 'subset'] <= 'small'].drop([98565,98567,98569,99134,108925,133297])\n",
    "tracks = tracks_.loc[subset].iloc[:N]\n",
    "\n",
    "## taking the different indexes for train, validation and testing\n",
    "train = tracks.index[tracks['set', 'split'] == 'training'].values\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation'].values\n",
    "test = tracks.index[tracks['set', 'split'] == 'test'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76813b10-e927-4bd9-b284-aad7364e145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the \"Y\" vectors (classification of the data) as onehot encoded\n",
    "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)\n",
    "number_genre = labels_onehot.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa67ca4-2008-47b1-ad1d-f4e7cd04ad3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a06786-6321-4326-a1c4-d363314d72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTLoader(utils.Loader):\n",
    "    raw_loader = utils.LibrosaLoader()\n",
    "\n",
    "    def load(self, filename):\n",
    "        import librosa\n",
    "        x = self.raw_loader.load(filename)\n",
    "        x_short = x[:max_len] ## getting only up to a certain length of the data sample (to have the same length for all data)\n",
    "        \n",
    "        stft = np.abs(librosa.stft(x_short, n_fft=n_fft, hop_length=hop_length))  ## n_fft is the size of the FFT,, hop_length is frame increment between \n",
    "        ## two consecutive FFT\n",
    "        return stft ,x# return a matrix (q,p) where p is the number of columns of the matrix (important afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dea1e12-2a6e-45a4-af92-55f5a06f2e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1321967,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_1d = utils.LibrosaLoader(sampling_rate=44100) ## 1d loader for raw audio\n",
    "loader_2d = STFTLoader()  # 2d loader for stft matrix\n",
    "loader_2d.load('fma/data/fma_small/000/000002.mp3')[1].shape  ##example for 2D data shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082377eb-8c34-45ae-b767-9061bd82d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_1d(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tids, onehot): ## tids = train, val or test. Are the indixes\n",
    "        'Initialization'\n",
    "        self.labels = onehot\n",
    "        self.tids = tids\n",
    "        self.loader = loader_1d\n",
    "        self.audio_path = utils.get_audio_path\n",
    "        self.audio_dir = AUDIO_DIR\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        y = self.labels.loc[self.tids[index]]\n",
    "        audio_name = self.audio_path(self.audio_dir, self.tids[index])\n",
    "        x = self.loader.load(audio_name)[:max_len]\n",
    "        \n",
    "        X_ = torch.from_numpy(np.array(x,dtype='float32'))\n",
    "        X = X_.reshape( *X_.shape,1)\n",
    "        Y = torch.from_numpy(np.array(y,dtype='float32'))\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9fa39a-091f-4cf5-8465-6f8c570b182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_2d(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tids, onehot): ## tids = train, val or test. Are the indixes\n",
    "        'Initialization'\n",
    "        self.labels = onehot\n",
    "        self.tids = tids\n",
    "        self.loader = loader_2d\n",
    "        self.audio_path = utils.get_audio_path\n",
    "        self.audio_dir = AUDIO_DIR\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        y = self.labels.loc[self.tids[index]]\n",
    "        audio_name = self.audio_path(self.audio_dir, self.tids[index])\n",
    "        x = self.loader.load(audio_name)[0]\n",
    "        \n",
    "        X = torch.from_numpy(np.array(x,dtype='float32')).permute(1,0)\n",
    "        Y = torch.from_numpy(np.array(y,dtype='float32'))\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8757342a-1264-4b9e-afbc-add974ebd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_both(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tids, onehot): ## tids = train, val or test. Are the indixes\n",
    "        'Initialization'\n",
    "        self.labels = onehot\n",
    "        self.tids = tids\n",
    "        self.loader = loader_2d\n",
    "        self.audio_path = utils.get_audio_path\n",
    "        self.audio_dir = AUDIO_DIR\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        y = self.labels.loc[self.tids[index]]\n",
    "        audio_name = self.audio_path(self.audio_dir, self.tids[index])\n",
    "        x_2, x_1  = self.loader.load(audio_name)\n",
    "        \n",
    "        X_1_ = torch.from_numpy(np.array(x_1,dtype='float32'))\n",
    "        X_2 = torch.from_numpy(np.array(x_2,dtype='float32')).permute(1,0)\n",
    "        X_1 = X_1_.reshape(*X_1_.shape ,1)[:max_len]\n",
    "        \n",
    "        Y = torch.from_numpy(np.array(y,dtype='float32'))\n",
    "\n",
    "        return X_1, X_2, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504255c-8f88-46f5-944b-fbd7d70e137e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d71359-da58-45ab-8a44-50df089556dd",
   "metadata": {},
   "source": [
    "#### 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5ce2d2-410d-44a2-ae09-6d42829a3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network1d(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_columns, kernel_size): ## kernell size has to be [a, n_rows] \n",
    "        super().__init__()\n",
    "        self.conv_path_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=256, kernel_size = [stride_calc*20, 1], stride = [stride_calc, 1], padding=[0, 0]), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[2,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[1,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=[n_columns-kernel_size[0]+1,1]) ## -(kernel_size[0]-1) since is the dim of conv path 1\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=[n_columns-kernel_size[0]+1,1])\n",
    "        \n",
    "        self.linear_path = nn.Sequential(\n",
    "            nn.Linear(in_features=256*2, out_features=300), ## in is n_fft*2 (2 different poolings)\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=300, out_features=150),\n",
    "            nn.BatchNorm1d(150),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=150, out_features=number_genre), ## out number of genres\n",
    "            nn.BatchNorm1d(number_genre),\n",
    "            nn.ELU()\n",
    "        )\n",
    "               \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1_1 = self.conv_path_1(x)\n",
    "        x1_2 = self.conv_path_2(x1_1)\n",
    "        x2 = x1_1 + x1_2\n",
    "        x3 = torch.cat((self.maxpool(x2),self.avgpool(x2)),2)\n",
    "        x4 = self.linear_path(self.flatten(x3))\n",
    "\n",
    "        \n",
    "        return x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6e22b-e2c2-4944-95a7-884d154decbc",
   "metadata": {},
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f1fd461-a46d-4f66-aa52-2ec1b3578f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_columns, kernel_size): ## kernell size has to be [a, n_rows] \n",
    "        super().__init__()\n",
    "        self.conv_path_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=256, kernel_size=kernel_size,padding=[0,0]), ## does a convolution over the width so yileds a 1d vector\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[2,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[1,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=[n_columns-kernel_size[0]+1,1]) ## -(kernel_size[0]-1) since is the dim of conv path 1\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=[n_columns-kernel_size[0]+1,1])\n",
    "        \n",
    "        self.linear_path = nn.Sequential(\n",
    "            nn.Linear(in_features=256*2, out_features=300), ## in is n_fft*2 (2 different poolings)\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=300, out_features=150),\n",
    "            nn.BatchNorm1d(150),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=150, out_features=number_genre), ## out number of genres\n",
    "            nn.BatchNorm1d(number_genre),\n",
    "            nn.ELU()\n",
    "        )\n",
    "               \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1_1 = self.conv_path_1(x)\n",
    "        x1_2 = self.conv_path_2(x1_1)\n",
    "        x2 = x1_1 + x1_2\n",
    "        x3 = torch.cat((self.maxpool(x2),self.avgpool(x2)),2)\n",
    "        x4 = self.linear_path(self.flatten(x3))\n",
    "\n",
    "        \n",
    "        return x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f606174-38b9-45de-8247-8726bfc419aa",
   "metadata": {},
   "source": [
    "#### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4db5d39-655b-42c4-a185-c42582e709ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkBoth(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_columns, kernel_size): ## kernell size has to be [a, n_rows] \n",
    "        super().__init__()\n",
    "        self.conv_path_1_2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=128, kernel_size=kernel_size,padding=[0,0]), ## does a convolution over the width so yileds a 1d vector\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_1_1d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=128, kernel_size = [stride_calc*20, 1], stride = [stride_calc, 1], padding=[0, 0]), ## convolution over the 1d input to match sizes\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.conv_path_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[2,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[4,1],padding=[1,0]), ##padding so it matches de dimension of conv_path 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=[n_columns-kernel_size[0]+1,1]) ## -(kernel_size[0]-1) since is the dim of conv path 1\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=[n_columns-kernel_size[0]+1,1])\n",
    "        \n",
    "        self.linear_path = nn.Sequential(\n",
    "            nn.Linear(in_features=256*2, out_features=300), ## in is n_fft*2 (2 different poolings)\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=300, out_features=150),\n",
    "            nn.BatchNorm1d(150),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(in_features=150, out_features=number_genre), ## out number of genres\n",
    "            nn.BatchNorm1d(number_genre),\n",
    "            nn.ELU()\n",
    "        )\n",
    "               \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x_1d, x_2d):\n",
    "        \n",
    "        x1_1d = self.conv_path_1_1d(x_1d)\n",
    "        x1_2d = self.conv_path_1_2d(x_2d)\n",
    "        x1_together = torch.cat((x1_1d, x1_2d), 1)\n",
    "        \n",
    "        x2 = self.conv_path_2(x1_together)\n",
    "        x3 = x1_together + x2\n",
    "        x4 = torch.cat((self.maxpool(x3),self.avgpool(x3)),2)\n",
    "        x5 = self.linear_path(self.flatten(x4))\n",
    "        \n",
    "        return x5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e730c8-02b1-48ed-89f0-1680b090f35e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e068a43-df4e-4df4-9893-350aea2d4541",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c178ac57-d4d5-486e-9289-b5cd37e4a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training for model 1d\n",
    "\n",
    "if model_dim == '1d':\n",
    "    model = Network1d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                \n",
    "    train_data = Dataset_1d(train, labels_onehot)\n",
    "    valid_data = Dataset_1d(val, labels_onehot)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "\n",
    "    opt = Adam(model.parameters(), lr = lr)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val = np.inf\n",
    "    \n",
    "    loss_train = np.array([])\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_avg = 0\n",
    "        loss_avg_val = 0\n",
    "        n_loss = 0\n",
    "        n_loss_val = 0\n",
    "        \n",
    "        model.train()\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        iterator = tqdm(train_loader)\n",
    "        for batch_x, batch_y in (iterator):\n",
    "            \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                batch_x_j = batch_x[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                \n",
    "                y_pred_j = model(batch_x_j) \n",
    "                loss = loss_fn(y_pred_j,batch_y) ## the loss is computed and the gradient is computed inside the for loop, for each segment\n",
    "                n_loss = n_loss+1\n",
    "\n",
    "                loss_avg = loss_avg + loss.detach().cpu().numpy()\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "    \n",
    "            \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            predictions = []\n",
    "            true = []\n",
    "            for batch_x, batch_y in tqdm(valid_loader):\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "    \n",
    "                batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "                \n",
    "    \n",
    "                y_pred = 0\n",
    "                for j in range(max([num_divisions*2-3,1])):\n",
    "                    \n",
    "                    batch_x_j = batch_x[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                    \n",
    "                    y_pred_j = model(batch_x_j)\n",
    "                    \n",
    "                    loss_avg_val = loss_avg_val + loss_fn(y_pred_j, batch_y).detach().cpu().numpy() ## this loss (inside) is to compare with training loss, is computed for each segment\n",
    "                    n_loss_val = n_loss_val + 1\n",
    "                    \n",
    "                    y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "                \n",
    "                predictions.append(y_pred) \n",
    "                true.append(batch_y)\n",
    "            predictions = torch.cat(predictions, axis=0)\n",
    "            true = torch.cat(true, axis=0)   ## the loss is computed OUTSIDE the for loop, once the predictions for the whole song is computed\n",
    "                                            ## this is the loss that we use to decide to save the model or not.\n",
    "            val_loss = loss_fn(predictions, true)\n",
    "    \n",
    "            index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "            zeros = np.zeros(predictions.shape)\n",
    "            zeros[np.arange(index.shape[0]), index]=1\n",
    "            \n",
    "            val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "                \n",
    "            print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "        loss_val_epoch.append(loss_avg_val/n_loss_val)\n",
    "        loss_train_epoch.append(loss_avg/n_loss)\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            print(\"Saved Model\")\n",
    "            torch.save(model.state_dict(), model_saving_name)\n",
    "            best_val = val_loss\n",
    "    \n",
    "    plt.plot(np.arange(1,epoch+2),loss_train_epoch,label='Training Loss')\n",
    "    plt.plot(np.arange(1,epoch+2),loss_val_epoch,label='Validation Loss (\"segment-wise\")')\n",
    "    plt.legend()\n",
    "\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28884fad-0656-4d15-9675-abbd0d8d9e33",
   "metadata": {},
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497a93e8-03ac-4b85-84e3-366770e705db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training for model 2d\n",
    "\n",
    "if model_dim == '2d':\n",
    "    model = Network2d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "    \n",
    "    train_data = Dataset_2d(train, labels_onehot)\n",
    "    valid_data = Dataset_2d(val, labels_onehot)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val = np.inf\n",
    "    \n",
    "    loss_train = np.array([])\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        loss_avg = 0\n",
    "        loss_avg_val = 0\n",
    "        n_loss = 0\n",
    "        n_loss_val = 0\n",
    "        \n",
    "        model.train()\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        iterator = tqdm(train_loader)\n",
    "        for batch_x, batch_y in (iterator):\n",
    "            \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                batch_x_j = batch_x[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "\n",
    "                y_pred_j = model(batch_x_j)\n",
    "                loss = loss_fn(y_pred_j,batch_y)\n",
    "\n",
    "                n_loss = n_loss+1\n",
    "                loss_avg = loss_avg + loss.detach().cpu().numpy()\n",
    "    \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "                \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            predictions = []\n",
    "            true = []\n",
    "            for batch_x, batch_y in tqdm(valid_loader):\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "    \n",
    "                batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "                \n",
    "    \n",
    "                y_pred = 0\n",
    "                for j in range(max([num_divisions*2-3,1])):\n",
    "                    \n",
    "                    batch_x_j = batch_x[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                    \n",
    "                    y_pred_j = model(batch_x_j)\n",
    "                    y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "                    \n",
    "                    loss_avg_val = loss_avg_val + loss_fn(y_pred_j, batch_y).detach().cpu().numpy()\n",
    "                    n_loss_val = n_loss_val + 1\n",
    "                \n",
    "                predictions.append(y_pred)\n",
    "                true.append(batch_y)\n",
    "            predictions = torch.cat(predictions, axis=0)\n",
    "            true = torch.cat(true, axis=0)\n",
    "            val_loss = loss_fn(predictions, true)\n",
    "    \n",
    "            index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "            zeros = np.zeros(predictions.shape)\n",
    "            zeros[np.arange(index.shape[0]), index]=1\n",
    "            \n",
    "            val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "                \n",
    "            print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "        loss_val_epoch.append(loss_avg_val/n_loss_val)\n",
    "        loss_train_epoch.append(loss_avg/n_loss)\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            print(\"Saved Model\")\n",
    "            torch.save(model.state_dict(), model_saving_name)\n",
    "            best_val = val_loss\n",
    "    plt.plot(np.arange(1,epoch+2),loss_train_epoch,label='Training Loss')\n",
    "    plt.plot(np.arange(1,epoch+2),loss_val_epoch,label='Validation Loss (\"segment-wise\")')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd4fe7-c49a-49fa-860c-3c91570adfc7",
   "metadata": {},
   "source": [
    "#### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6aded63-b616-4e75-ad20-9c0fa0fda2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3987348079681396:   7%|███▌                                               | 9/128 [01:14<16:53,  8.52s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 1.2879455089569092:  55%|███████████████████████████▋                      | 71/128 [15:59<17:32, 18.46s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.3654448986053467:  62%|███████████████████████████████▎                  | 80/128 [20:30<33:32, 41.92s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.2963497638702393:  66%|████████████████████████████████▊                 | 84/128 [21:15<13:25, 18.30s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.278822660446167: 100%|██████████████████████████████████████████████████| 128/128 [30:30<00:00, 14.30s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:16<00:00,  8.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5180672407150269, accuracy: 0.46875\n",
      "Saved Model\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3055742979049683:  41%|████████████████████▎                             | 52/128 [08:21<11:34,  9.14s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 1.2405339479446411:  45%|██████████████████████▎                           | 57/128 [09:08<11:03,  9.35s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.307918667793274:  45%|███████████████████████                            | 58/128 [09:22<12:24, 10.64s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 1.219225287437439:  79%|███████████████████████████████████████▍          | 101/128 [16:30<04:13,  9.40s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.2145391702651978:  84%|████████████████████████████████████████▉        | 107/128 [17:26<03:09,  9.03s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.5067636966705322: 100%|█████████████████████████████████████████████████| 128/128 [21:24<00:00, 10.04s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:06<00:00,  7.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5542808771133423, accuracy: 0.4525\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0164968967437744:  21%|██████████▌                                       | 27/128 [04:48<15:23,  9.14s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.9792197942733765:  45%|██████████████████████▋                           | 58/128 [09:32<10:27,  8.97s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.06291663646698:  52%|██████████████████████████▊                         | 66/128 [10:49<08:59,  8.70s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.1084208488464355:  62%|███████████████████████████████▎                  | 80/128 [13:06<07:27,  9.33s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 1.3877356052398682:  66%|█████████████████████████████████▏                | 85/128 [13:51<06:22,  8.89s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.2788852453231812: 100%|█████████████████████████████████████████████████| 128/128 [19:57<00:00,  9.35s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [01:57<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5127630233764648, accuracy: 0.46875\n",
      "Saved Model\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8831986784934998:  20%|██████████▏                                       | 26/128 [03:24<13:23,  7.87s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 1.003764271736145:  45%|███████████████████████                            | 58/128 [07:45<10:00,  8.58s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.9417845010757446:  49%|████████████████████████▌                         | 63/128 [08:29<09:27,  8.72s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.109771728515625:  80%|███████████████████████████████████████▊          | 102/128 [14:08<03:30,  8.11s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.018315076828003:  87%|███████████████████████████████████████████▎      | 111/128 [15:24<02:24,  8.50s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.98268061876297: 100%|███████████████████████████████████████████████████| 128/128 [17:52<00:00,  8.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [01:52<00:00,  7.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4462952613830566, accuracy: 0.52125\n",
      "Saved Model\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7920695543289185:  12%|█████▊                                            | 15/128 [02:06<15:12,  8.07s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 1.1007760763168335:  30%|███████████████▏                                  | 39/128 [05:25<12:18,  8.30s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.9507651925086975:  49%|████████████████████████▌                         | 63/128 [09:05<09:11,  8.49s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.8540819883346558:  84%|█████████████████████████████████████████▎       | 108/128 [15:49<03:21, 10.09s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.0405362844467163:  91%|████████████████████████████████████████████▍    | 116/128 [17:04<01:55,  9.62s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 1.0467913150787354: 100%|█████████████████████████████████████████████████| 128/128 [18:57<00:00,  8.89s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [03:32<00:00, 13.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4674230813980103, accuracy: 0.49875\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6576273441314697:  12%|██████▎                                           | 16/128 [02:43<21:10, 11.34s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.8967636823654175:  18%|████████▉                                         | 23/128 [04:10<19:08, 10.94s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.0613987445831299:  23%|███████████▎                                      | 29/128 [05:13<17:06, 10.36s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.7560284733772278:  36%|█████████████████▉                                | 46/128 [07:56<12:40,  9.27s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.8516413569450378:  99%|████████████████████████████████████████████████▌| 127/128 [21:00<00:08,  8.28s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.9202449321746826: 100%|█████████████████████████████████████████████████| 128/128 [21:08<00:00,  9.91s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:19<00:00,  8.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4630428552627563, accuracy: 0.5\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8392539620399475:   2%|█▏                                                 | 3/128 [00:27<18:52,  9.06s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 1.0038466453552246:  10%|█████                                             | 13/128 [02:01<17:31,  9.14s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.8684617280960083:  20%|█████████▊                                        | 25/128 [04:08<22:30, 13.11s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.7356181740760803:  55%|███████████████████████████▎                      | 70/128 [13:05<09:20,  9.66s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.9784448146820068:  74%|█████████████████████████████████████             | 95/128 [17:15<05:00,  9.12s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.7648196220397949: 100%|█████████████████████████████████████████████████| 128/128 [22:30<00:00, 10.55s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:12<00:00,  8.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.556207299232483, accuracy: 0.51375\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6831521391868591:  43%|█████████████████████▍                            | 55/128 [08:05<11:05,  9.12s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.6038593649864197:  62%|██████████████████████████████▊                   | 79/128 [12:05<07:10,  8.78s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.6239266991615295:  65%|████████████████████████████████▍                 | 83/128 [12:43<07:01,  9.36s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.5640795230865479:  70%|██████████████████████████████████▊               | 89/128 [13:42<06:19,  9.73s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.47709712386131287:  86%|█████████████████████████████████████████▎      | 110/128 [17:02<02:49,  9.44s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.6129588484764099: 100%|█████████████████████████████████████████████████| 128/128 [19:52<00:00,  9.32s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:10<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5870018005371094, accuracy: 0.505\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5312548875808716:  11%|█████▍                                            | 14/128 [02:07<17:13,  9.06s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.5459818840026855:  22%|██████████▉                                       | 28/128 [04:21<15:32,  9.33s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.3900283873081207:  39%|███████████████████▌                              | 50/128 [07:49<13:13, 10.17s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.858734667301178:  55%|███████████████████████████▉                       | 70/128 [10:55<08:53,  9.19s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.6832009553909302:  76%|█████████████████████████████████████▉            | 97/128 [15:15<04:44,  9.18s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.6298818588256836: 100%|█████████████████████████████████████████████████| 128/128 [20:15<00:00,  9.49s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:10<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.651961326599121, accuracy: 0.50125\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5093519687652588:  21%|██████████▌                                       | 27/128 [03:57<14:35,  8.67s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.4173365831375122:  28%|██████████████                                    | 36/128 [05:21<15:03,  9.82s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.524900496006012:  30%|███████████████▌                                   | 39/128 [05:51<14:26,  9.74s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.3436858654022217:  48%|███████████████████████▊                          | 61/128 [09:13<10:56,  9.79s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.3727826476097107:  92%|█████████████████████████████████████████████▏   | 118/128 [19:00<02:20, 14.07s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.4426802098751068: 100%|█████████████████████████████████████████████████| 128/128 [21:06<00:00,  9.90s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:19<00:00,  8.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5868428945541382, accuracy: 0.4825\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4996599555015564:  46%|███████████████████████                           | 59/128 [09:21<10:20,  8.99s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.5310693383216858:  52%|█████████████████████████▊                        | 66/128 [10:31<09:53,  9.57s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.45894545316696167:  53%|██████████████████████████                       | 68/128 [10:52<10:06, 10.11s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.4507814049720764:  73%|████████████████████████████████████▎             | 93/128 [15:03<05:22,  9.21s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.602437436580658:  83%|█████████████████████████████████████████▍        | 106/128 [17:18<03:46, 10.31s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.7134933471679688: 100%|█████████████████████████████████████████████████| 128/128 [20:44<00:00,  9.72s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:04<00:00,  7.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6569173336029053, accuracy: 0.5\n",
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3215249180793762:   2%|▊                                                  | 2/128 [00:16<16:41,  7.95s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.19021235406398773:  20%|█████████▉                                       | 26/128 [03:55<15:17,  8.99s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.3517390489578247:  36%|█████████████████▉                                | 46/128 [06:58<12:22,  9.05s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.29036280512809753:  37%|█████████████████▉                               | 47/128 [07:08<12:34,  9.32s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.418807715177536:  71%|████████████████████████████████████▎              | 91/128 [14:07<06:14, 10.12s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.4076405465602875: 100%|█████████████████████████████████████████████████| 128/128 [19:50<00:00,  9.30s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:05<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.702966570854187, accuracy: 0.43875\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2722472846508026:  20%|██████████▏                                       | 26/128 [03:59<14:48,  8.71s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.4105435013771057:  27%|█████████████▋                                    | 35/128 [05:21<14:15,  9.20s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.4593428075313568:  46%|███████████████████████                           | 59/128 [09:10<10:10,  8.85s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.42399367690086365:  62%|██████████████████████████████▋                  | 80/128 [12:24<07:35,  9.49s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.2102770209312439:  67%|█████████████████████████████████▌                | 86/128 [13:17<06:14,  8.92s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.1594190001487732: 100%|█████████████████████████████████████████████████| 128/128 [20:13<00:00,  9.48s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:12<00:00,  8.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9353164434432983, accuracy: 0.4575\n",
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18277910351753235:   6%|███▏                                              | 8/128 [01:16<19:24,  9.70s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.24078205227851868:  18%|████████▊                                        | 23/128 [04:07<20:38, 11.80s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.22713151574134827:  35%|█████████████████▏                               | 45/128 [07:39<12:26,  9.00s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.2954351603984833:  41%|████████████████████▎                             | 52/128 [08:53<13:58, 11.03s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.23717521131038666:  41%|████████████████████▎                            | 53/128 [09:02<12:55, 10.34s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.2587701082229614: 100%|█████████████████████████████████████████████████| 128/128 [21:29<00:00, 10.07s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:11<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.79318368434906, accuracy: 0.47375\n",
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09828455001115799:  19%|█████████▏                                       | 24/128 [03:47<15:21,  8.86s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Train loss: 0.2693048417568207:  34%|████████████████▊                                 | 43/128 [07:02<16:17, 11.50s/it][src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Train loss: 0.12689664959907532:  36%|█████████████████▌                               | 46/128 [07:37<15:49, 11.58s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.2070230394601822:  70%|██████████████████████████████████▊               | 89/128 [14:34<05:50,  8.98s/it][src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n",
      "Train loss: 0.32208526134490967:  96%|██████████████████████████████████████████████▏ | 123/128 [22:19<01:25, 17.03s/it][src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Train loss: 0.17018191516399384: 100%|████████████████████████████████████████████████| 128/128 [23:17<00:00, 10.91s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [02:22<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9234191179275513, accuracy: 0.455\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_dim == 'both':\n",
    "    model = NetworkBoth(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                \n",
    "    train_data = Dataset_both(train, labels_onehot)\n",
    "    valid_data = Dataset_both(val, labels_onehot)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "\n",
    "    opt = Adam(model.parameters(),lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "  \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val = np.inf\n",
    "    \n",
    "    loss_train = np.array([])\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_avg = 0\n",
    "        loss_avg_val = 0\n",
    "        n_loss = 0\n",
    "        n_loss_val = 0\n",
    "        model.train()\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        iterator = tqdm(train_loader)\n",
    "        for batch_x1, batch_x2, batch_y in (iterator):\n",
    "            \n",
    "            batch_x1 = batch_x1.to(device)\n",
    "            batch_x2 = batch_x2.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            batch_x1 = batch_x1.reshape(1, *batch_x1.shape).permute(1,0,2,3)\n",
    "            batch_x2 = batch_x2.reshape(1, *batch_x2.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "                \n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                batch_x_j1 = batch_x1[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                batch_x_j2 = batch_x2[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]\n",
    "                \n",
    "                y_pred_j = model(batch_x_j1, batch_x_j2)\n",
    "        \n",
    "                loss = loss_fn(y_pred_j,batch_y)\n",
    "\n",
    "                n_loss = n_loss+1\n",
    "                loss_avg = loss_avg + loss.detach().cpu().numpy()\n",
    "        \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
    "                \n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true = []\n",
    "            for batch_x1, batch_x2, batch_y in (tqdm(valid_loader)):\n",
    "                \n",
    "                batch_x1 = batch_x1.to(device)\n",
    "                batch_x2 = batch_x2.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "            \n",
    "                batch_x1 = batch_x1.reshape(1, *batch_x1.shape).permute(1,0,2,3)\n",
    "                batch_x2 = batch_x2.reshape(1, *batch_x2.shape).permute(1,0,2,3)\n",
    "                \n",
    "                y_pred = 0\n",
    "                \n",
    "                for j in range(max([num_divisions*2-3,1])):\n",
    "                    batch_x_j1 = batch_x1[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                    batch_x_j2 = batch_x2[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]\n",
    "                    \n",
    "                    y_pred_j = model(batch_x_j1, batch_x_j2)\n",
    "                    y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "\n",
    "                    loss_avg_val = loss_avg_val + loss_fn(y_pred_j, batch_y).detach().cpu().numpy()\n",
    "                    n_loss_val = n_loss_val + 1\n",
    "    \n",
    "                predictions.append(y_pred)\n",
    "                true.append(batch_y)\n",
    "                    \n",
    "            predictions = torch.cat(predictions, axis=0)\n",
    "            true = torch.cat(true, axis=0)\n",
    "            val_loss = loss_fn(predictions,true)\n",
    "    \n",
    "            index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "            zeros = np.zeros(predictions.shape)\n",
    "            zeros[np.arange(index.shape[0]), index]=1\n",
    "            \n",
    "            val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "                \n",
    "            print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "\n",
    "        loss_val_epoch.append(loss_avg_val/n_loss_val)\n",
    "        loss_train_epoch.append(loss_avg/n_loss)\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            print(\"Saved Model\")\n",
    "            torch.save(model.state_dict(), model_saving_name)\n",
    "            best_val = val_loss\n",
    "    plt.plot(np.arange(1,epoch+2),loss_train_epoch,label='Training Loss')\n",
    "    plt.plot(np.arange(1,epoch+2),loss_val_epoch,label='Validation Loss (\"segment-wise\")')\n",
    "    plt.legend()\n",
    "\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522724e-da80-4891-b1ad-59e6e079eae3",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54350ab8-5fec-4261-9669-69f14d551ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model.pt' #Name of the model we want to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e341f0a-8897-4c60-8aed-f782fcd3c6bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f34b580-5f7b-4979-8e86-47ea00c725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dim == '1d':\n",
    "    model = Network1d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                 \n",
    "    test_data = Dataset_1d(test, labels_onehot)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(test_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "\n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                \n",
    "                batch_x_j = batch_x[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                \n",
    "                y_pred_j = model(batch_x_j)\n",
    "                y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "            \n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "\n",
    "        index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "        zeros = np.zeros(predictions.shape)\n",
    "        zeros[np.arange(index.shape[0]), index]=1\n",
    "        \n",
    "        val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "            \n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12216f-dfc9-46bd-a18b-35de90f00309",
   "metadata": {},
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce0170dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dim == '2d':\n",
    "    model = Network2d(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                 \n",
    "    test_data = Dataset_2d(test, labels_onehot)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(test_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            batch_x = batch_x.reshape(1, *batch_x.shape).permute(1,0,2,3)\n",
    "            \n",
    "\n",
    "            y_pred = 0\n",
    "            for j in range(max([num_divisions*2-3,1])):\n",
    "                \n",
    "                batch_x_j = batch_x[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                \n",
    "                y_pred_j = model(batch_x_j)\n",
    "                y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "            \n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "\n",
    "        index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "        zeros = np.zeros(predictions.shape)\n",
    "        zeros[np.arange(index.shape[0]), index]=1\n",
    "        \n",
    "        val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "            \n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b5a32-7de0-439c-979f-e0065a96416c",
   "metadata": {},
   "source": [
    "#### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "560f688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [01:55<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7352166175842285, accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "if model_dim == 'both':\n",
    "    model = NetworkBoth(batch_size, n_columns=n_columns_division, kernel_size=[4,n_rows])\n",
    "                                                                                 \n",
    "    test_data = Dataset_both(test, labels_onehot)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x1, batch_x2, batch_y in (tqdm(test_loader)):\n",
    "            \n",
    "            batch_x1 = batch_x1.to(device)\n",
    "            batch_x2 = batch_x2.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "        \n",
    "            batch_x1 = batch_x1.reshape(1, *batch_x1.shape).permute(1,0,2,3)\n",
    "            batch_x2 = batch_x2.reshape(1, *batch_x2.shape).permute(1,0,2,3)\n",
    "            \n",
    "            y_pred = 0\n",
    "            \n",
    "            for j in range(num_divisions*2-3):\n",
    "                batch_x_j1 = batch_x1[:,:,int(n_data_division*j/2):int(n_data_division*(j+2)/2),:]  ## 50% of overlap between pieces of data\n",
    "                batch_x_j2 = batch_x2[:,:,int(n_columns_division*j/2):int(n_columns_division*(j+2)/2),:]\n",
    "                \n",
    "                y_pred_j = model(batch_x_j1, batch_x_j2)\n",
    "                y_pred = y_pred + y_pred_j/max([num_divisions*2-3,1])\n",
    "\n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "                \n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "\n",
    "        index = torch.argmax(predictions,axis=1).detach().cpu().numpy()\n",
    "        zeros = np.zeros(predictions.shape)\n",
    "        zeros[np.arange(index.shape[0]), index]=1\n",
    "        \n",
    "        val_acc = np.sum(np.equal(np.sum(np.equal(zeros,true.detach().cpu().numpy()).astype(int),axis=1),number_genre).astype(int))/predictions.shape[0]\n",
    "            \n",
    "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
    "else:\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e142-ad51-459b-b5d3-832e118310d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
